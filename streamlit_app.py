# ==============================================================================
#  "Tek DosyalÄ±k GÃ¼Ã§ Merkezi" MVP (v37.0)
# ==============================================================================
import streamlit as st
import pandas as pd
import requests
import feedparser
import folium
from streamlit_folium import folium_static
from openai import OpenAI
import json
import re
from urllib.parse import quote, urlparse
from datetime import datetime
from rapidfuzz import fuzz

# Metin Ã‡Ä±karma KÃ¼tÃ¼phaneleri
import trafilatura
from bs4 import BeautifulSoup
import readability

# ------------------------------------------------------------------------------
# 1. TEMEL AYARLAR VE API BAÄLANTILARI
# ------------------------------------------------------------------------------
st.set_page_config(layout="wide", page_title="EndÃ¼striyel Hasar Motoru")
st.title("ğŸ›°ï¸ AkÄ±llÄ± EndÃ¼striyel Hasar Motoru")

# --- API BaÄŸlantÄ±larÄ± (Streamlit Secrets'ten) ---
grok_api_key = st.secrets.get("GROK_API_KEY")
google_api_key = st.secrets.get("GOOGLE_MAPS_API_KEY")
client = OpenAI(api_key=grok_api_key, base_url="https://api.x.ai/v1") if grok_api_key else None

# ------------------------------------------------------------------------------
# 2. VERÄ°TABANI KURULUMU VE YÃ–NETÄ°MÄ° (DAHÄ°LÄ°)
# ------------------------------------------------------------------------------
@st.cache_resource
def get_db_connection():
    # Streamlit'in kendi baÄŸlantÄ± yÃ¶netimi ile SQLite veritabanÄ± oluÅŸtur/baÄŸlan
    return st.connection("events_db", type="sql", url="sqlite:///events.db")

conn = get_db_connection()

# Uygulama ilk Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda veritabanÄ± tablosunu oluÅŸtur
with conn.session as s:
    s.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY,
            headline TEXT,
            url TEXT UNIQUE,
            source TEXT,
            published_date TEXT,
            full_text TEXT,
            status TEXT DEFAULT 'new',
            report_json TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );
    """)
    s.commit()

# ------------------------------------------------------------------------------
# 3. YARDIMCI FONKSÄ°YONLAR (METÄ°N Ã‡EKME, AI ZÄ°NCÄ°RÄ°, HARÄ°TA)
# ------------------------------------------------------------------------------

# -- Metin Ã‡Ä±karma Motoru --
@st.cache_data(ttl=86400) # Bir URL'yi gÃ¼nde bir defadan fazla Ã§ekme
def fetch_article_text(url: str) -> str:
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15)
        resp.raise_for_status()
        html_doc = resp.text
        
        # 1. Katman: Trafilatura
        extracted = trafilatura.extract(html_doc, include_comments=False, include_tables=False)
        if extracted and len(extracted.strip()) > 300: return extracted.strip()

        # 2. Katman: Readability
        doc = readability.Document(html_doc)
        text = BeautifulSoup(doc.summary(), "lxml").get_text(separator="\n", strip=True)
        if text and len(text.strip()) > 300: return text.strip()

        # 3. Katman: BeautifulSoup Fallback
        soup = BeautifulSoup(html_doc, "lxml")
        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        text = "\n".join([p for p in paragraphs if len(p) > 50])
        return text.strip()
    except Exception:
        return ""

# -- AI Analiz Zinciri --
def extract_clues_from_text(_client, full_text):
    prompt = f"GÃ–REV: AÅŸaÄŸÄ±daki haber metnini oku ve X'te arama yapmak iÃ§in kullanÄ±labilecek en spesifik ipuÃ§larÄ±nÄ± Ã§Ä±kar. Ã‡Ä±ktÄ±yÄ± SADECE JSON formatÄ±nda ver.\nHABER METNÄ°: \"{full_text[:8000]}\"\n\nJSON YAPISI: {{\"en_spesifik_konum\": \"...\", \"tesis_tipi\": \"...\", \"olay_detaylari\": \"...\"}}"
    response = _client.chat.completions.create(model="grok-4-fast-reasoning", messages=[{"role": "user", "content": prompt}], max_tokens=1024, temperature=0.0)
    match = re.search(r'\{.*\}', response.choices[0].message.content, re.DOTALL)
    return json.loads(match.group(0)) if match else {}

def simulate_x_search_for_name(_client, clues):
    prompt = f"Sen bir OSINT uzmanÄ±sÄ±n. GÃ¶revin, sana verilen ipuÃ§larÄ±nÄ± kullanarak X (Twitter) Ã¼zerinde nokta atÄ±ÅŸÄ± bir arama simÃ¼lasyonu yapmak ve olayÄ±n yaÅŸandÄ±ÄŸÄ± tesisin ticari unvanÄ±nÄ± teyit etmektir. ASLA TAHMÄ°N YÃœRÃœTME.\nÄ°PUÃ‡LARI: {clues}\n\nÃ‡IKTI FORMATI (SADECE JSON): {{\"tesis_adi\": \"YÃ¼ksek kesinlikle bulunan isim VEYA 'Teyit Edilemedi'\", \"kanit\": \"Ä°smi nasÄ±l bulduÄŸunun aÃ§Ä±klamasÄ±.\"}}"
    response = _client.chat.completions.create(model="grok-4-fast-reasoning", messages=[{"role": "user", "content": prompt}], max_tokens=2048, temperature=0.0)
    match = re.search(r'\{.*\}', response.choices[0].message.content, re.DOTALL)
    return json.loads(match.group(0)) if match else {}

def generate_final_report(_client, full_text, company_info):
    prompt = f"Sen elit bir sigorta risk analistisin. Bilgiler ÅŸunlar:\n- TEYÄ°T EDÄ°LMÄ°Å TESÄ°S BÄ°LGÄ°SÄ°: {company_info}\n- OLAYIN HABER METNÄ°: \"{full_text[:8000]}\"\n\nGÃ–REVÄ°N: Bu bilgileri kullanarak, aÅŸaÄŸÄ±daki tÃ¼m anahtarlarÄ± dolduran nihai ve detaylÄ± JSON raporunu oluÅŸtur.\n\nJSON YAPISI: \"sehir_ilce\", \"tahmini_koordinat\": {{\"lat\": \"...\", \"lon\": \"...\"}}, \"maddi_hasar_fiziksel_boyut\", \"is_durmasi_kar_kaybi\", \"hasarin_nedeni\", \"yapilan_mudahale\", \"guncel_durum\", \"cevreye_etki\", \"gorsel_url\""
    response = _client.chat.completions.create(model="grok-4-fast-reasoning", messages=[{"role": "user", "content": prompt}], max_tokens=4096, temperature=0.1)
    match = re.search(r'\{.*\}', response.choices[0].message.content, re.DOTALL)
    final_data = json.loads(match.group(0)) if match else {}
    final_data.update(company_info)
    return final_data

# -- Harita Fonksiyonu --
def find_neighboring_facilities(api_key, lat, lon):
    if not all([api_key, lat, lon]): return []
    try:
        url = f"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={lat},{lon}&radius=1000&type=establishment&keyword=fabrika|depo|sanayi|tesis&key={api_key}"
        response = requests.get(url)
        results = response.json().get('results', [])
        return [{"tesis_adi": p.get('name'), "adres": p.get('vicinity'), "lat": p.get('geometry',{}).get('location',{}).get('lat'),"lng": p.get('geometry',{}).get('location',{}).get('lng')} for p in results[:10]]
    except Exception: return []

# ------------------------------------------------------------------------------
# 4. ANA Ä°Å AKIÅI VE STREAMLIT ARAYÃœZÃœ
# ------------------------------------------------------------------------------

# --- Sol SÃ¼tun: Kontrol Paneli ---
with st.sidebar:
    st.header("Kontrol Paneli")
    if st.button("ğŸ“° Yeni OlaylarÄ± Tara", type="primary", use_container_width=True):
        with st.spinner("Haber kaynaklarÄ± taranÄ±yor ve veritabanÄ±na ekleniyor..."):
            locations = '"fabrika" OR "sanayi" OR "OSB" OR "liman" OR "depo"'
            events = '"yangÄ±n" OR "patlama" OR "kaza" OR "sÄ±zÄ±ntÄ±"'
            q = f'({locations}) AND ({events})'
            rss_url = f"https://news.google.com/rss/search?q={quote(q)}+when:3d&hl=tr&gl=TR&ceid=TR:tr"
            
            feed = feedparser.parse(rss_url)
            
            # VeritabanÄ±ndaki mevcut baÅŸlÄ±klarÄ± Ã§ek
            existing_headlines_df = conn.query("SELECT headline FROM articles;")
            existing_headlines = existing_headlines_df['headline'].tolist()

            new_articles_count = 0
            for entry in feed.entries:
                headline = entry.title.split(" - ")[0]
                url = entry.link
                source = urlparse(url).netloc
                
                # Benzerlik kontrolÃ¼
                is_similar = any(fuzz.ratio(headline, old_headline) > 85 for old_headline in existing_headlines)

                if not is_similar:
                    full_text = fetch_article_text(url)
                    if full_text and len(full_text) > 300:
                        # VeritabanÄ±na yeni makaleyi ekle
                        with conn.session as s:
                            s.execute(
                                "INSERT INTO articles (headline, url, source, published_date, full_text, status) VALUES (:headline, :url, :source, :published, :text, 'new') ON CONFLICT(url) DO NOTHING;",
                                params={"headline": headline, "url": url, "source": entry.get("published"), "published": entry.get("published"), "text": full_text}
                            )
                            s.commit()
                        existing_headlines.append(headline)
                        new_articles_count += 1
        st.success(f"{new_articles_count} yeni olay adayÄ± veritabanÄ±na eklendi.")
        st.rerun()

    st.markdown("---")
    
    # Analiz bekleyen olaylarÄ± veritabanÄ±ndan Ã§ek
    new_events_df = conn.query("SELECT id, headline FROM articles WHERE status = 'new' ORDER BY id DESC;")
    if not new_events_df.empty:
        st.subheader("Analiz Bekleyen Olaylar")
        event_options = {f"{row['headline']} (ID: {row['id']})": row['id'] for index, row in new_events_df.iterrows()}
        selected_event_display = st.radio("Bir olay seÃ§in:", event_options.keys())
        st.session_state.selected_event_id = event_options[selected_event_display]
    else:
        st.info("Analiz bekleyen yeni olay bulunmuyor.")

# --- SaÄŸ SÃ¼tun: Analiz ve Rapor Paneli ---
if 'selected_event_id' in st.session_state:
    event_id = st.session_state.selected_event_id
    
    # SeÃ§ilen olayÄ±n tÃ¼m bilgilerini veritabanÄ±ndan al
    event_data = conn.query(f"SELECT * FROM articles WHERE id = {event_id};", ttl=0).iloc[0]
    
    st.header("Analiz Paneli")
    st.subheader(event_data['headline'])
    st.caption(f"Kaynak: {event_data['source']} | [Haber Linki]({event_data['url']})")

    if event_data['status'] == 'new':
        if st.button("âœ… Bu OlayÄ± Analiz Et", type="primary", use_container_width=True):
            with st.status("Ä°stihbarat Analisti ProtokolÃ¼ yÃ¼rÃ¼tÃ¼lÃ¼yor...", expanded=True) as status:
                full_text = event_data['full_text']
                
                status.write("AÅŸama 1: Haber metninden ipuÃ§larÄ± Ã§Ä±karÄ±lÄ±yor...")
                clues = extract_clues_from_text(client, full_text)
                
                status.write("AÅŸama 2: Ä°puÃ§larÄ± ile X'te kontrollÃ¼ arama simÃ¼le ediliyor...")
                company_info = simulate_x_search_for_name(client, clues)
                
                status.write("AÅŸama 3: Nihai rapor oluÅŸturuluyor...")
                report_data = generate_final_report(client, full_text, company_info)
                
                status.write("AÅŸama 4: Rapor coÄŸrafi verilerle zenginleÅŸtiriliyor...")
                coords = report_data.get('tahmini_koordinat', {})
                lat, lon = coords.get('lat'), coords.get('lon')
                if lat and lon:
                   report_data['komsu_tesisler'] = find_neighboring_facilities(google_api_key, lat, lon)
                
                # Raporu ve durumu veritabanÄ±na kaydet
                with conn.session as s:
                    s.execute(
                        "UPDATE articles SET status = 'processed', report_json = :report WHERE id = :id;",
                        params={"report": json.dumps(report_data, ensure_ascii=False), "id": event_id}
                    )
                    s.commit()
                
                status.update(label="Analiz BaÅŸarÄ±yla TamamlandÄ±!", state="complete", expanded=False)
                st.rerun()

    # EÄŸer olay daha Ã¶nce analiz edildiyse, raporu veritabanÄ±ndan gÃ¶ster
    if event_data['status'] == 'processed':
        st.success("Bu olay daha Ã¶nce analiz edildi. Rapor aÅŸaÄŸÄ±dadÄ±r.")
        report = json.loads(event_data['report_json'])
        
        st.subheader(f"Rapor: {report.get('tesis_adi', 'Ä°simsiz Tesis')}")
        st.info(f"**KanÄ±t:** *\"{report.get('kanit', 'KanÄ±t bulunamadÄ±.')}\"*")
        
        # ... (Rapor gÃ¶sterme kodunun geri kalanÄ±, Ã¶nceki versiyonlarla aynÄ±) ...
        with st.expander("DetaylÄ± Raporu GÃ¶rÃ¼ntÃ¼le", expanded=True):
            if report.get('gorsel_url') and 'http' in report.get('gorsel_url'):
                st.image(report['gorsel_url'])
            
            st.markdown("##### Hasar Analizi")
            col_m, col_k = st.columns(2)
            with col_m: st.warning(f"**Maddi Hasar (Fiziksel Boyut):** {report.get('maddi_hasar_fiziksel_boyut', 'Detay Yok')}")
            with col_k: st.error(f"**Ä°ÅŸ DurmasÄ± / Kar KaybÄ±:** {report.get('is_durmasi_kar_kaybi', 'Detay Yok')}")

            st.markdown("##### Olay YÃ¶netimi ve Etkileri")
            st.success(f"**YapÄ±lan MÃ¼dahale:** {report.get('yapilan_mudahale', 'Detay Yok')}")
            st.info(f"**GÃ¼ncel Durum:** {report.get('guncel_durum', 'Detay Yok')}")

            coords = report.get('tahmini_koordinat', {})
            lat, lon = coords.get('lat'), coords.get('lon')
            if lat and lon:
                try:
                    m = folium.Map(location=[float(lat), float(lon)], zoom_start=14, tiles="CartoDB positron")
                    folium.Marker([float(lat), float(lon)], popup=f"<b>{report.get('tesis_adi')}</b>", icon=folium.Icon(color='red', icon='fire')).add_to(m)
                    neighbors = report.get('komsu_tesisler', [])
                    for n in neighbors:
                        if n.get('lat') and n.get('lng'):
                            folium.Marker([n['lat'], n['lng']], popup=f"<b>{n['tesis_adi']}</b>", icon=folium.Icon(color='blue', icon='industry', prefix='fa')).add_to(m)
                    folium_static(m, height=400)
                    st.table(pd.DataFrame(neighbors)[['tesis_adi', 'adres']])
                except (ValueError, TypeError): st.warning("Harita Ã§izilemiyor.")
