# -*- coding: utf-8 -*-
import os
import re
import json
import time
import math
import requests
import feedparser
import pandas as pd
from urllib.parse import urlparse
from datetime import datetime, timedelta, timezone
from dateutil import parser as dateparser
from unidecode import unidecode
import streamlit as st
from openai import OpenAI

# -------------------------------
# APP CONFIG
# -------------------------------
st.set_page_config(page_title="Tesis AdÄ± Ã‡Ä±karÄ±mÄ± (Grok)", layout="wide")
st.title("ğŸ­ Tesis AdÄ± Ã‡Ä±karÄ±mÄ± â€“ Grok OdaklÄ± (Yerel + X doÄŸrulama yÃ¶nlendirmeli)")

XAI_API_KEY = os.getenv("XAI_API_KEY")
if not XAI_API_KEY:
    st.error("XAI_API_KEY ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil.")
    st.stop()

client = OpenAI(api_key=XAI_API_KEY, base_url="https://api.x.ai/v1")

DEFAULT_MODEL = "grok-3"  # eriÅŸiminize gÃ¶re "grok-4" veya "grok-4-fast-reasoning" seÃ§ebilirsiniz
LOCAL_HINTS = [
    ".bel.tr",".gov.tr",".edu.tr",".k12.tr",".osb",".osb.org",".org.tr",
    "haber","sondakika","yerel","manset","gazete","kent","kenthaber","medya"
]

TR_CITIES = [
    "Adana","AdÄ±yaman","Afyonkarahisar","AÄŸrÄ±","Aksaray","Amasya","Ankara","Antalya","Ardahan","Artvin",
    "AydÄ±n","BalÄ±kesir","BartÄ±n","Batman","Bayburt","Bilecik","BingÃ¶l","Bitlis","Bolu","Burdur",
    "Bursa","Ã‡anakkale","Ã‡ankÄ±rÄ±","Ã‡orum","Denizli","DiyarbakÄ±r","DÃ¼zce","Edirne","ElazÄ±ÄŸ","Erzincan",
    "Erzurum","EskiÅŸehir","Gaziantep","Giresun","GÃ¼mÃ¼ÅŸhane","HakkÃ¢ri","Hatay","IÄŸdÄ±r","Isparta","Ä°stanbul",
    "Ä°zmir","KahramanmaraÅŸ","KarabÃ¼k","Karaman","Kars","Kastamonu","Kayseri","KÄ±rÄ±kkale","KÄ±rklareli","KÄ±rÅŸehir",
    "Kilis","Kocaeli","Konya","KÃ¼tahya","Malatya","Manisa","Mardin","Mersin","MuÄŸla","MuÅŸ",
    "NevÅŸehir","NiÄŸde","Ordu","Osmaniye","Rize","Sakarya","Samsun","Siirt","Sinop","Sivas",
    "ÅanlÄ±urfa","ÅÄ±rnak","TekirdaÄŸ","Tokat","Trabzon","Tunceli","UÅŸak","Van","Yalova","Yozgat","Zonguldak"
]
CITY_RX = re.compile(r"\b(" + "|".join([re.escape(c) for c in TR_CITIES]) + r")\b", re.IGNORECASE)

def norm(s: str) -> str:
    if not s: return ""
    s = unidecode(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# -------------------------------
# RSS FETCH (deterministik)
# -------------------------------
def google_news_rss_query(keywords: str, days: int = 7) -> str:
    q = requests.utils.quote(f'({keywords}) when:{days}d')
    # TR odaklÄ±; ulusal kaynaklar gelebilir ama sonra filtreleyeceÄŸiz
    return f"https://news.google.com/rss/search?q={q}&hl=tr&gl=TR&ceid=TR:tr"

def fetch_rss(keyword_list, days_back=7, max_per_kw=20):
    items = []
    for kw in keyword_list:
        url = google_news_rss_query(kw, days_back)
        feed = feedparser.parse(url)
        for e in feed.entries[:max_per_kw]:
            link = e.get("link","")
            title = e.get("title","")
            summary = e.get("summary","")
            published = e.get("published","") or e.get("updated","")
            try:
                dt = dateparser.parse(published)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
            except Exception:
                dt = None
            items.append({
                "kw": kw,
                "title": title,
                "summary": summary,
                "link": link,
                "published": dt
            })
        time.sleep(0.4)  # nazik
    return items

def domain_score(u: str) -> int:
    try:
        d = urlparse(u).netloc.lower()
    except Exception:
        return 0
    # Yerel ipucu iÃ§eren domainleri Ã¶ne Ã§ek
    score = 0
    for hint in LOCAL_HINTS:
        if hint in d:
            score += 2
    # bÃ¼yÃ¼k ulusal domainleri biraz kÄ±rp (ajanslar yerine yerel kaynak istiyoruz)
    if any(x in d for x in ["ntv","cnnturk","hurriyet","sozcu","sabah","aa.com.tr","dha.com.tr"]):
        score -= 1
    return score

def rank_and_pick(items, start_dt, end_dt, limit=30, only_local=True):
    # tarih aralÄ±ÄŸÄ±
    items = [it for it in items if isinstance(it["published"], datetime) and start_dt <= it["published"] <= end_dt]
    # skorla
    for it in items:
        it["score"] = domain_score(it["link"])
    if only_local:
        items = [it for it in items if it["score"] >= 1]
    # taze + puanlÄ± sÄ±rala
    items = sorted(items, key=lambda x: (x["score"], x["published"]), reverse=True)
    return items[:limit]

# -------------------------------
# GROK: TESIS ADI Ã‡IKARIMI
# -------------------------------
SCHEMA_EXAMPLE = [
  {
    "olay_tarihi": "2025-09-18",
    "sehir": "Kayseri",
    "ilce": "Melikgazi",
    "tesis_adi": "Ã–rnek Tekstil A.Å. FabrikasÄ±",
    "tesis_ad_teyit": "TEYITLI | TAHMIN | TEYIT_EDILEMEDI",
    "kanit_linkleri": ["https://yerelgazete.com.tr/...", "https://x.com/..."],
    "aciklama": "Yerel haberde tesisin unvanÄ± aÃ§Ä±kÃ§a geÃ§iyor. X hesabÄ±nda itfaiye teyidi var.",
    "guven_skoru": 0.92
  }
]

SYSTEM_PROMPT = """
Sen bir sigorta uzmanÄ±sÄ±n ve canlÄ± bilgiye eriÅŸebilen bir araÅŸtÄ±rmacÄ± gibi davranacaksÄ±n.
AÅŸaÄŸÄ±da verilen HABER LÄ°STESÄ°, TÃ¼rkiye'deki endÃ¼striyel/sanayi olaylarÄ±na dair linklerden oluÅŸur.
GÃ¶revin: her linki **mÃ¼mkÃ¼nse yerel haber ve X paylaÅŸÄ±mlarÄ± ile teyit ederek** olay bazÄ±nda TESÄ°S ADI'nÄ± Ã§Ä±karmak.
Kurallar:
- Tesis adÄ± net ve aÃ§Ä±k unvanla geÃ§iyorsa "tesis_ad_teyit" = TEYITLI.
- Sadece metinden tahmin ediliyorsa "TAHMIN".
- BulunamadÄ±ysa "TEYIT_EDILEMEDI".
- MÃ¼mkÃ¼nse en az 2 **kanÄ±t_linki** ver (yerel haber + X postu/itfaiye/valilik).
- Sadece TÃ¼rkiye iÃ§indeki olaylar, son gÃ¼nlerdeki haberler.
- YanÄ±t SADECE JSON dizi olsun; baÅŸka metin yok.
- JSON alanlarÄ±: olay_tarihi, sehir, ilce, tesis_adi, tesis_ad_teyit, kanit_linkleri, aciklama, guven_skoru.
DÄ±ÅŸ doÄŸrulama (web/X) yapabiliyorsan yap; yapamÄ±yorsan link iÃ§eriÄŸine ve baÅŸlÄ±ÄŸa dayan.
"""

def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

@st.cache_data(show_spinner=False, ttl=900)
def grok_extract_facilities(model, batch_items):
    # batch_items: list of dict {title, summary, link, published}
    payload = []
    for it in batch_items:
        payload.append({
            "title": unidecode(re.sub(r"<.*?>","", it["title"] or ""))[:220],
            "summary": unidecode(re.sub(r"<.*?>","", it["summary"] or ""))[:600],
            "link": it["link"],
            "published": it["published"].strftime("%Y-%m-%d %H:%M") if isinstance(it["published"], datetime) else ""
        })
    messages = [
        {"role":"system","content": SYSTEM_PROMPT.strip()},
        {"role":"user","content": json.dumps({"haber_listesi": payload}, ensure_ascii=False)}
    ]
    resp = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=2200,
        temperature=0.0
    )
    content = (resp.choices[0].message.content or "").strip()
    m = re.search(r"\[.*\]\s*$", content, re.DOTALL)
    if not m:
        return []
    try:
        data = json.loads(m.group(0))
        # ÅŸema gÃ¼venliÄŸi
        out = []
        for d in data:
            out.append({
                "olay_tarihi": d.get("olay_tarihi"),
                "sehir": d.get("sehir"),
                "ilce": d.get("ilce"),
                "tesis_adi": d.get("tesis_adi"),
                "tesis_ad_teyit": d.get("tesis_ad_teyit"),
                "kanit_linkleri": d.get("kanit_linkleri", []),
                "aciklama": d.get("aciklama"),
                "guven_skoru": d.get("guven_skoru")
            })
        return out
    except Exception:
        return []

# -------------------------------
# UI â€“ Parametreler
# -------------------------------
with st.sidebar:
    st.header("Sorgu AyarlarÄ±")
    today = datetime.now(timezone.utc).date()
    end_date = st.date_input("BitiÅŸ Tarihi", value=today)
    days_back = st.slider("Geri GÃ¼n", 1, 30, 7)
    start_dt = datetime.combine(end_date - timedelta(days=days_back), datetime.min.time()).replace(tzinfo=timezone.utc)
    end_dt = datetime.combine(end_date, datetime.max.time()).replace(tzinfo=timezone.utc)

    st.markdown("**Anahtar Kelimeler (satÄ±r satÄ±r)**")
    default_kws = [
        "fabrika yangÄ±n", "tesis yangÄ±n", "sanayi yangÄ±nÄ±", "OSB yangÄ±n",
        "patlama fabrika", "kimyasal sÄ±zÄ±ntÄ±", "rafineri yangÄ±n", "trafo patlamasÄ±"
    ]
    kw_text = st.text_area("Kelimeler", "\n".join(default_kws), height=150)
    only_local = st.checkbox("Sadece yerel domainleri Grok'a gÃ¶nder", value=True)
    max_links = st.slider("Grok'a gÃ¶nderilecek link sayÄ±sÄ± (toplam)", 5, 60, 25, step=5)
    batch_size = st.slider("Batch boyutu (API tasarrufu)", 5, 25, 15, step=5)
    model_name = st.selectbox("Grok Modeli", [DEFAULT_MODEL, "grok-4", "grok-4-fast-reasoning"], index=0)

run = st.button("Tesis AdlarÄ±nÄ± Ã‡Ä±kar (Grok)")

st.markdown("---")

# -------------------------------
# Ã‡alÄ±ÅŸtÄ±rma
# -------------------------------
if run:
    keywords = [k.strip() for k in kw_text.splitlines() if k.strip()]
    with st.spinner("RSS kaynaklarÄ± Ã§ekiliyor..."):
        raw = fetch_rss(keywords, days_back=days_back, max_per_kw=20)
    st.success(f"RSS'ten {len(raw)} aday haber geldi.")

    picked = rank_and_pick(raw, start_dt, end_dt, limit=max_links, only_local=only_local)
    if not picked:
        st.warning("SeÃ§ilen aralÄ±k/filtre ile uygun yerel link bulunamadÄ±. 'Sadece yerel' tikini kapatÄ±p tekrar deneyin.")
        st.stop()

    st.info(f"Grok'a gÃ¶nderilecek link sayÄ±sÄ±: {len(picked)} (batch={batch_size})")
    df_src = pd.DataFrame([{
        "published": it["published"],
        "domain": urlparse(it["link"]).netloc if it["link"] else "",
        "title": it["title"], "link": it["link"]
    } for it in picked])
    st.dataframe(df_src, use_container_width=True)

    results = []
    for chunk in chunks(picked, batch_size):
        with st.spinner(f"Grok Ã§Ä±karÄ±m yapÄ±yor... ({len(chunk)} link)"):
            out = grok_extract_facilities(model_name, chunk)
            results.extend(out)
        # nazik bekleme (kotalarÄ± zorlamamak iÃ§in)
        time.sleep(1.0)

    if not results:
        st.warning("Grok, verilen linklerden tesis adÄ± Ã§Ä±karamadÄ±. Link sayÄ±sÄ±nÄ± artÄ±rÄ±n veya 'Sadece yerel' filtresini kapatÄ±p yeniden deneyin.")
        st.stop()

    df = pd.DataFrame(results)
    # Kolon dÃ¼zeni
    for c in ["olay_tarihi","sehir","ilce","tesis_adi","tesis_ad_teyit","guven_skoru","aciklama","kanit_linkleri"]:
        if c not in df.columns:
            df[c] = None

    st.subheader("ğŸ” Tesis AdÄ± Ã‡Ä±karÄ±mÄ± â€“ SonuÃ§lar")
    st.dataframe(df[["olay_tarihi","sehir","ilce","tesis_adi","tesis_ad_teyit","guven_skoru","aciklama"]], use_container_width=True)

    st.subheader("ğŸ”— KanÄ±t Linkleri")
    for i, row in df.iterrows():
        links = row.get("kanit_linkleri") or []
        if links:
            st.markdown(f"**Olay {i+1} â€“ {row.get('tesis_adi') or 'Tesis?'}**")
            for L in links[:6]:
                st.markdown(f"- {L}")

    st.success("Tamam. Sonraki adÄ±mda bu sonuÃ§larÄ± PD/BI analizi ve haritalama iÃ§in kullanabiliriz.")
else:
    st.info("Parametreleri seÃ§ip 'Tesis AdlarÄ±nÄ± Ã‡Ä±kar (Grok)' butonuna basÄ±n. Ä°lk adÄ±mda sadece tesis adÄ± tespiti yapÄ±lÄ±r; API tÃ¼ketimi batching ile sÄ±nÄ±rlÄ±dÄ±r.")
